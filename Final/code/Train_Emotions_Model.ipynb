{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from imutils import paths\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init variables\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 50\n",
    "BS = 32\n",
    "\n",
    "imagePaths = list(paths.list_images(\"Resources//data3\"))\n",
    "imagePaths = [imagePath.replace(\"\\\\\",\"//\",-1) for imagePath in imagePaths]\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # Extract the class label from directory\n",
    "    label = imagePath.split(\"//\")[2]\n",
    "    labels.append(label)\n",
    "    \n",
    "    # Convert image to grey scale and 48x48\n",
    "    image = load_img(imagePath, color_mode = \"grayscale\", target_size=(48, 48, 1))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    data.append(image)\n",
    "\n",
    "# Convert the data and labels to np arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)    \n",
    "\n",
    "# Perform one-hot encoding on labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS),metrics=[\"accuracy\"])\n",
    "history = model.fit(trainX,trainY,batch_size=BS,epochs=EPOCHS,validation_data=(testX, testY))\n",
    "\n",
    "model.save(\"Resources/Trained_Data/test2.h5\")\n",
    "\n",
    "print(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predIdxs = model.predict(testX, batch_size=BS)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch No.\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"Resources/Training_Graph\")\n",
    "print(\"Training Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define data generators\n",
    "# # train_dir = 'Resources/test'\n",
    "# # val_dir = 'Resources/train'\n",
    "# # num_train = 28709\n",
    "# # num_val = 7178\n",
    "# # batch_num = 64\n",
    "# # num_epoch = 50\n",
    "\n",
    "# train_dir = 'Resources/data'\n",
    "# val_dir = 'Resources/data2'\n",
    "# num_train = 5\n",
    "# num_val = 5\n",
    "# batch_size = 1\n",
    "# num_epoch = 5\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#         train_dir,\n",
    "#         target_size=(48,48),\n",
    "#         batch_size=batch_size,\n",
    "#         color_mode=\"grayscale\",\n",
    "#         class_mode='categorical')\n",
    "\n",
    "# validation_generator = val_datagen.flow_from_directory(\n",
    "#         val_dir,\n",
    "#         target_size=(48,48),\n",
    "#         batch_size=batch_size,\n",
    "#         color_mode=\"grayscale\",\n",
    "#         class_mode='categorical')\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "\n",
    "# model_info = model.fit(train_generator, steps_per_epoch=num_train // batch_size, \n",
    "#                                  epochs=num_epoch,validation_data=validation_generator, validation_steps=num_val // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(\"Resources/cascade/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "model2 = tf.keras.models.clone_model(model)\n",
    "\n",
    "model.load_weights('Resources/Trained_Data/test1.h5')\n",
    "model2.load_weights('Resources/Trained_Data/test2.h5')\n",
    "\n",
    "# dictionary which assigns each label an emotion (alphabetical order)\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"Resources/images/faces.jpg\")\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = faceCascade.detectMultiScale(img, 1.3, 5)\n",
    "\n",
    "img2 = img.copy()\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
    "    roi_gray=gray[y:y+w,x:x+h]\n",
    "    roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "    img_pixels = image.img_to_array(roi_gray)\n",
    "    img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "    img_pixels /= 255\n",
    "    \n",
    "    prediction = model.predict(img_pixels)\n",
    "    maxindex = int(np.argmax(prediction))\n",
    "    cv2.putText(img, emotion_dict[maxindex], (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.rectangle(img2,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
    "    prediction2 = model2.predict(img_pixels)\n",
    "    maxindex2 = int(np.argmax(prediction2))\n",
    "    cv2.putText(img2, emotion_dict[maxindex2], (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    #prediction, idx, probability = learn.predict(Image(pil2tensor(roi_gray, np.float32).div_(225)))\n",
    "    #cv2.putText(img, str(prediction), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (225, 255, 255), 2)\n",
    "\n",
    "resized_img = cv2.resize(img, (1000, 700))\n",
    "cv2.imshow('Facial emotion analysis ',resized_img)\n",
    "\n",
    "\n",
    "resized_img2 = cv2.resize(img2, (1000, 700))\n",
    "cv2.imshow('Facial emotion analysis2 ',resized_img2)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
